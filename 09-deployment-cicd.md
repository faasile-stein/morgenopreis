# Stage 9: CI/CD & Deployment

## Overview
Establish production-ready CI/CD pipelines, deployment strategies, monitoring, and operational runbooks for TravelTomorrow platform.

## Objectives
- Set up automated CI/CD pipelines
- Configure production infrastructure
- Implement monitoring and alerting
- Create deployment strategies (blue-green, canary)
- Establish operational runbooks
- Configure backup and disaster recovery

---

## 1. CI/CD Pipeline Architecture

### GitHub Actions Workflow Structure

```
.github/workflows/
â”œâ”€â”€ api-ci.yml              # API tests and lint
â”œâ”€â”€ web-ci.yml              # Web tests and build
â”œâ”€â”€ mobile-ci.yml           # Mobile app build
â”œâ”€â”€ drupal-ci.yml           # Drupal tests
â”œâ”€â”€ deploy-staging.yml      # Deploy to staging
â”œâ”€â”€ deploy-production.yml   # Deploy to production
â”œâ”€â”€ database-migration.yml  # Database migrations
â””â”€â”€ security-scan.yml       # Security scanning
```

---

## 2. Main CI/CD Pipeline

**.github/workflows/main-pipeline.yml:**
```yaml
name: Main CI/CD Pipeline

on:
  push:
    branches: [main, develop, 'claude/**']
  pull_request:
    branches: [main, develop]

env:
  NODE_VERSION: '20'
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  # Lint and Type Check
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Lint
        run: npm run lint

      - name: Type check
        run: npm run type-check

  # Unit and Integration Tests
  test:
    runs-on: ubuntu-latest
    needs: lint

    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_DB: test_db
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_pass
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run migrations
        run: npm run migrate:test
        env:
          DATABASE_URL: postgresql://test_user:test_pass@localhost:5432/test_db

      - name: Run tests
        run: npm run test -- --coverage
        env:
          DATABASE_URL: postgresql://test_user:test_pass@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379
          NODE_ENV: test

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          flags: api,web

  # E2E Tests
  e2e:
    runs-on: ubuntu-latest
    needs: test

    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright
        run: npx playwright install --with-deps

      - name: Run E2E tests
        run: npm run test:e2e

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: playwright-report
          path: playwright-report/

  # Build Docker Images
  build:
    runs-on: ubuntu-latest
    needs: [test, e2e]
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop')

    permissions:
      contents: read
      packages: write

    steps:
      - uses: actions/checkout@v4

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}

      - name: Build and push API image
        uses: docker/build-push-action@v5
        with:
          context: ./packages/api
          push: true
          tags: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/api:${{ github.sha }}
          labels: ${{ steps.meta.outputs.labels }}

      - name: Build and push Web image
        uses: docker/build-push-action@v5
        with:
          context: ./packages/web
          push: true
          tags: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/web:${{ github.sha }}

  # Deploy to Staging
  deploy-staging:
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/develop'
    environment: staging

    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-west-1

      - name: Deploy to ECS
        run: |
          aws ecs update-service \
            --cluster traveltomorrow-staging \
            --service api \
            --force-new-deployment

      - name: Run database migrations
        run: |
          # Run migrations via ECS task
          aws ecs run-task \
            --cluster traveltomorrow-staging \
            --task-definition migrate-staging \
            --launch-type FARGATE

  # Deploy to Production
  deploy-production:
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/main'
    environment: production

    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-west-1

      - name: Blue-Green Deployment
        run: |
          # Get current task definition
          CURRENT_TASK_DEF=$(aws ecs describe-services \
            --cluster traveltomorrow-prod \
            --services api \
            --query 'services[0].taskDefinition' \
            --output text)

          # Create new task definition with new image
          NEW_TASK_DEF=$(aws ecs register-task-definition \
            --cli-input-json file://task-definition.json \
            --query 'taskDefinition.taskDefinitionArn' \
            --output text)

          # Update service to use new task definition
          aws ecs update-service \
            --cluster traveltomorrow-prod \
            --service api \
            --task-definition $NEW_TASK_DEF

          # Wait for deployment to complete
          aws ecs wait services-stable \
            --cluster traveltomorrow-prod \
            --services api

      - name: Notify Slack
        uses: slackapi/slack-github-action@v1
        with:
          webhook-url: ${{ secrets.SLACK_WEBHOOK }}
          payload: |
            {
              "text": "ðŸš€ Production deployment completed successfully"
            }
```

---

## 3. Infrastructure as Code (Terraform)

### Production Infrastructure

**infrastructure/terraform/production/main.tf:**
```hcl
terraform {
  required_version = ">= 1.0"

  backend "s3" {
    bucket = "traveltomorrow-terraform-state"
    key    = "production/terraform.tfstate"
    region = "eu-west-1"
  }
}

provider "aws" {
  region = "eu-west-1"
}

# VPC
module "vpc" {
  source = "terraform-aws-modules/vpc/aws"

  name = "traveltomorrow-prod"
  cidr = "10.0.0.0/16"

  azs             = ["eu-west-1a", "eu-west-1b", "eu-west-1c"]
  private_subnets = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
  public_subnets  = ["10.0.101.0/24", "10.0.102.0/24", "10.0.103.0/24"]

  enable_nat_gateway = true
  enable_vpn_gateway = false
  enable_dns_hostnames = true

  tags = {
    Environment = "production"
    Project     = "TravelTomorrow"
  }
}

# RDS PostgreSQL
module "rds" {
  source = "terraform-aws-modules/rds/aws"

  identifier = "traveltomorrow-prod"

  engine               = "postgres"
  engine_version       = "15.4"
  family               = "postgres15"
  instance_class       = "db.t3.large"

  allocated_storage     = 100
  max_allocated_storage = 500

  db_name  = "traveltomorrow"
  username = "dbadmin"
  port     = 5432

  multi_az               = true
  db_subnet_group_name   = module.vpc.database_subnet_group_name
  vpc_security_group_ids = [aws_security_group.rds.id]

  backup_retention_period = 30
  backup_window          = "03:00-04:00"
  maintenance_window     = "Mon:04:00-Mon:05:00"

  deletion_protection = true
  skip_final_snapshot = false

  performance_insights_enabled = true

  tags = {
    Environment = "production"
  }
}

# ElastiCache Redis
resource "aws_elasticache_replication_group" "redis" {
  replication_group_id       = "traveltomorrow-prod"
  replication_group_description = "Redis cluster for TravelTomorrow"

  engine               = "redis"
  engine_version       = "7.0"
  node_type            = "cache.t3.medium"
  num_cache_clusters   = 2

  automatic_failover_enabled = true
  multi_az_enabled          = true

  subnet_group_name  = aws_elasticache_subnet_group.redis.name
  security_group_ids = [aws_security_group.redis.id]

  at_rest_encryption_enabled = true
  transit_encryption_enabled = true

  snapshot_retention_limit = 5
  snapshot_window         = "03:00-05:00"

  tags = {
    Environment = "production"
  }
}

# ECS Cluster
resource "aws_ecs_cluster" "main" {
  name = "traveltomorrow-prod"

  setting {
    name  = "containerInsights"
    value = "enabled"
  }

  tags = {
    Environment = "production"
  }
}

# ECS Task Definition - API
resource "aws_ecs_task_definition" "api" {
  family                   = "traveltomorrow-api"
  network_mode             = "awsvpc"
  requires_compatibilities = ["FARGATE"]
  cpu                      = "1024"
  memory                   = "2048"

  execution_role_arn = aws_iam_role.ecs_execution_role.arn
  task_role_arn      = aws_iam_role.ecs_task_role.arn

  container_definitions = jsonencode([
    {
      name  = "api"
      image = "${var.ecr_repository_url}/api:latest"

      portMappings = [
        {
          containerPort = 3001
          protocol      = "tcp"
        }
      ]

      environment = [
        { name = "NODE_ENV", value = "production" },
        { name = "PORT", value = "3001" },
      ]

      secrets = [
        {
          name      = "DATABASE_URL"
          valueFrom = "${aws_secretsmanager_secret.app_secrets.arn}:database_url::"
        },
        {
          name      = "REDIS_URL"
          valueFrom = "${aws_secretsmanager_secret.app_secrets.arn}:redis_url::"
        },
      ]

      logConfiguration = {
        logDriver = "awslogs"
        options = {
          "awslogs-group"         = "/ecs/traveltomorrow-api"
          "awslogs-region"        = "eu-west-1"
          "awslogs-stream-prefix" = "ecs"
        }
      }

      healthCheck = {
        command     = ["CMD-SHELL", "curl -f http://localhost:3001/health || exit 1"]
        interval    = 30
        timeout     = 5
        retries     = 3
        startPeriod = 60
      }
    }
  ])
}

# ECS Service - API
resource "aws_ecs_service" "api" {
  name            = "api"
  cluster         = aws_ecs_cluster.main.id
  task_definition = aws_ecs_task_definition.api.arn
  desired_count   = 3

  launch_type = "FARGATE"

  network_configuration {
    subnets          = module.vpc.private_subnets
    security_groups  = [aws_security_group.ecs_tasks.id]
    assign_public_ip = false
  }

  load_balancer {
    target_group_arn = aws_lb_target_group.api.arn
    container_name   = "api"
    container_port   = 3001
  }

  deployment_configuration {
    maximum_percent         = 200
    minimum_healthy_percent = 100

    deployment_circuit_breaker {
      enable   = true
      rollback = true
    }
  }

  enable_execute_command = true

  tags = {
    Environment = "production"
  }
}

# Application Load Balancer
resource "aws_lb" "main" {
  name               = "traveltomorrow-alb"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [aws_security_group.alb.id]
  subnets            = module.vpc.public_subnets

  enable_deletion_protection = true
  enable_http2              = true

  tags = {
    Environment = "production"
  }
}

# CloudFront Distribution
resource "aws_cloudfront_distribution" "web" {
  enabled             = true
  is_ipv6_enabled     = true
  comment             = "TravelTomorrow Web App"
  default_root_object = "index.html"

  origin {
    domain_name = aws_s3_bucket.web_assets.bucket_regional_domain_name
    origin_id   = "S3-web-assets"

    s3_origin_config {
      origin_access_identity = aws_cloudfront_origin_access_identity.web.cloudfront_access_identity_path
    }
  }

  default_cache_behavior {
    allowed_methods  = ["GET", "HEAD", "OPTIONS"]
    cached_methods   = ["GET", "HEAD"]
    target_origin_id = "S3-web-assets"

    forwarded_values {
      query_string = false
      cookies {
        forward = "none"
      }
    }

    viewer_protocol_policy = "redirect-to-https"
    min_ttl                = 0
    default_ttl            = 3600
    max_ttl                = 86400
    compress               = true
  }

  restrictions {
    geo_restriction {
      restriction_type = "none"
    }
  }

  viewer_certificate {
    acm_certificate_arn      = aws_acm_certificate.web.arn
    ssl_support_method       = "sni-only"
    minimum_protocol_version = "TLSv1.2_2021"
  }

  tags = {
    Environment = "production"
  }
}

# Auto Scaling
resource "aws_appautoscaling_target" "api" {
  max_capacity       = 10
  min_capacity       = 3
  resource_id        = "service/${aws_ecs_cluster.main.name}/${aws_ecs_service.api.name}"
  scalable_dimension = "ecs:service:DesiredCount"
  service_namespace  = "ecs"
}

resource "aws_appautoscaling_policy" "api_cpu" {
  name               = "api-cpu-autoscaling"
  policy_type        = "TargetTrackingScaling"
  resource_id        = aws_appautoscaling_target.api.resource_id
  scalable_dimension = aws_appautoscaling_target.api.scalable_dimension
  service_namespace  = aws_appautoscaling_target.api.service_namespace

  target_tracking_scaling_policy_configuration {
    predefined_metric_specification {
      predefined_metric_type = "ECSServiceAverageCPUUtilization"
    }
    target_value = 70.0
  }
}
```

---

## 4. Monitoring & Alerting

### CloudWatch Alarms

**infrastructure/terraform/monitoring.tf:**
```hcl
# API High CPU Alarm
resource "aws_cloudwatch_metric_alarm" "api_cpu_high" {
  alarm_name          = "traveltomorrow-api-cpu-high"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 2
  metric_name         = "CPUUtilization"
  namespace           = "AWS/ECS"
  period              = 300
  statistic           = "Average"
  threshold           = 80

  dimensions = {
    ClusterName = aws_ecs_cluster.main.name
    ServiceName = aws_ecs_service.api.name
  }

  alarm_actions = [aws_sns_topic.alerts.arn]
}

# API High Memory Alarm
resource "aws_cloudwatch_metric_alarm" "api_memory_high" {
  alarm_name          = "traveltomorrow-api-memory-high"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 2
  metric_name         = "MemoryUtilization"
  namespace           = "AWS/ECS"
  period              = 300
  statistic           = "Average"
  threshold           = 80

  dimensions = {
    ClusterName = aws_ecs_cluster.main.name
    ServiceName = aws_ecs_service.api.name
  }

  alarm_actions = [aws_sns_topic.alerts.arn]
}

# RDS High CPU Alarm
resource "aws_cloudwatch_metric_alarm" "rds_cpu_high" {
  alarm_name          = "traveltomorrow-rds-cpu-high"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 2
  metric_name         = "CPUUtilization"
  namespace           = "AWS/RDS"
  period              = 300
  statistic           = "Average"
  threshold           = 80

  dimensions = {
    DBInstanceIdentifier = module.rds.db_instance_id
  }

  alarm_actions = [aws_sns_topic.alerts.arn]
}

# SNS Topic for Alerts
resource "aws_sns_topic" "alerts" {
  name = "traveltomorrow-alerts"
}

resource "aws_sns_topic_subscription" "alerts_email" {
  topic_arn = aws_sns_topic.alerts.arn
  protocol  = "email"
  endpoint  = var.alert_email
}
```

### Application Performance Monitoring (DataDog)

**packages/api/src/instrumentation/datadog.ts:**
```typescript
import tracer from 'dd-trace';

if (process.env.NODE_ENV === 'production') {
  tracer.init({
    service: 'traveltomorrow-api',
    env: process.env.NODE_ENV,
    version: process.env.APP_VERSION,
    logInjection: true,
    runtimeMetrics: true,
  });
}

export default tracer;
```

---

## 5. Database Migration Strategy

### Migration Pipeline

**.github/workflows/database-migration.yml:**
```yaml
name: Database Migration

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to run migration'
        required: true
        type: choice
        options:
          - staging
          - production

jobs:
  migrate:
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}

    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: npm ci

      - name: Run migrations
        run: npm run migrate:prod
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}

      - name: Verify migration
        run: npx prisma db status
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}

      - name: Notify on failure
        if: failure()
        uses: slackapi/slack-github-action@v1
        with:
          webhook-url: ${{ secrets.SLACK_WEBHOOK }}
          payload: |
            {
              "text": "âŒ Database migration failed in ${{ inputs.environment }}"
            }
```

---

## 6. Backup & Disaster Recovery

### Automated Backups

**scripts/backup.sh:**
```bash
#!/bin/bash

# Database Backup Script

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backups/postgres"
S3_BUCKET="s3://traveltomorrow-backups"

# Create backup
pg_dump $DATABASE_URL | gzip > "${BACKUP_DIR}/backup_${TIMESTAMP}.sql.gz"

# Upload to S3
aws s3 cp "${BACKUP_DIR}/backup_${TIMESTAMP}.sql.gz" \
  "${S3_BUCKET}/daily/backup_${TIMESTAMP}.sql.gz"

# Clean up old local backups (keep last 7 days)
find $BACKUP_DIR -name "backup_*.sql.gz" -mtime +7 -delete

echo "Backup completed: backup_${TIMESTAMP}.sql.gz"
```

### Disaster Recovery Runbook

**docs/runbooks/disaster-recovery.md:**
```markdown
# Disaster Recovery Runbook

## Database Failure

1. **Assess the situation**
   - Check RDS console for status
   - Check CloudWatch metrics
   - Review recent changes

2. **Failover to replica** (if multi-AZ)
   ```bash
   aws rds failover-db-cluster \
     --db-cluster-identifier traveltomorrow-prod
   ```

3. **Restore from backup** (if total failure)
   ```bash
   # Get latest backup
   aws rds describe-db-snapshots \
     --db-instance-identifier traveltomorrow-prod \
     --query 'DBSnapshots[-1].DBSnapshotIdentifier'

   # Restore from snapshot
   aws rds restore-db-instance-from-db-snapshot \
     --db-instance-identifier traveltomorrow-prod-restored \
     --db-snapshot-identifier <snapshot-id>
   ```

## Complete Service Outage

1. **Activate DR site**
2. **Update DNS to point to DR**
3. **Verify all services operational**
4. **Communicate with users**

## Recovery Time Objectives (RTO)

- Database: 1 hour
- API Services: 30 minutes
- Web Application: 15 minutes
```

---

## 7. Deployment Scripts

**scripts/deploy.sh:**
```bash
#!/bin/bash

set -e

ENVIRONMENT=$1
VERSION=$2

if [ -z "$ENVIRONMENT" ] || [ -z "$VERSION" ]; then
  echo "Usage: ./deploy.sh <environment> <version>"
  exit 1
fi

echo "Deploying version $VERSION to $ENVIRONMENT..."

# Build and push Docker images
docker build -t traveltomorrow/api:$VERSION ./packages/api
docker push traveltomorrow/api:$VERSION

# Update ECS task definition
aws ecs register-task-definition \
  --cli-input-json file://task-definition-$ENVIRONMENT.json

# Update service
aws ecs update-service \
  --cluster traveltomorrow-$ENVIRONMENT \
  --service api \
  --force-new-deployment

# Wait for deployment
aws ecs wait services-stable \
  --cluster traveltomorrow-$ENVIRONMENT \
  --services api

echo "Deployment complete!"
```

---

## 8. Health Checks & Readiness Probes

**packages/api/src/routes/health.ts:**
```typescript
import { Router } from 'express';
import { prisma } from '../config/database';
import { redis } from '../config/redis';
import { duffelClient } from '@traveltomorrow/shared/clients/duffel';

const router = Router();

// Liveness probe (basic health check)
router.get('/health', (req, res) => {
  res.json({
    status: 'ok',
    timestamp: new Date().toISOString(),
    uptime: process.uptime(),
  });
});

// Readiness probe (check dependencies)
router.get('/health/ready', async (req, res) => {
  const checks = {
    database: false,
    redis: false,
    duffel: false,
  };

  try {
    // Check database
    await prisma.$queryRaw`SELECT 1`;
    checks.database = true;
  } catch (error) {
    console.error('Database health check failed:', error);
  }

  try {
    // Check Redis
    await redis.ping();
    checks.redis = true;
  } catch (error) {
    console.error('Redis health check failed:', error);
  }

  try {
    // Check Duffel API
    await duffelClient.airlines.list({ limit: 1 });
    checks.duffel = true;
  } catch (error) {
    console.error('Duffel health check failed:', error);
  }

  const allHealthy = Object.values(checks).every(Boolean);
  const statusCode = allHealthy ? 200 : 503;

  res.status(statusCode).json({
    status: allHealthy ? 'ready' : 'not ready',
    checks,
    timestamp: new Date().toISOString(),
  });
});

export default router;
```

---

## 9. Operational Runbooks

### Service Restart Runbook

**docs/runbooks/service-restart.md:**
```markdown
# Service Restart Runbook

## API Service Restart

```bash
# Restart API service
aws ecs update-service \
  --cluster traveltomorrow-prod \
  --service api \
  --force-new-deployment

# Monitor deployment
aws ecs describe-services \
  --cluster traveltomorrow-prod \
  --services api
```

## Clear Redis Cache

```bash
# Connect to Redis
redis-cli -h <redis-endpoint> --tls

# Flush specific keys
KEYS offer:*
DEL offer:*

# Or flush all (use with caution)
FLUSHALL
```

## Database Connection Issues

```bash
# Check active connections
psql $DATABASE_URL -c "SELECT count(*) FROM pg_stat_activity;"

# Kill idle connections
psql $DATABASE_URL -c "
  SELECT pg_terminate_backend(pid)
  FROM pg_stat_activity
  WHERE state = 'idle'
  AND state_change < current_timestamp - INTERVAL '10 minutes';
"
```
```

---

## Deliverables

- [ ] CI/CD pipelines configured
- [ ] Production infrastructure deployed (Terraform)
- [ ] Monitoring and alerting set up
- [ ] Automated backups configured
- [ ] Disaster recovery plan documented
- [ ] Deployment scripts created
- [ ] Health checks implemented
- [ ] Operational runbooks written
- [ ] Security scanning in CI/CD

## Success Criteria

1. âœ… Automated deployments to staging and production
2. âœ… Zero-downtime deployments
3. âœ… Monitoring covers all critical metrics
4. âœ… Backups tested and verified
5. âœ… RTO < 1 hour for critical services
6. âœ… All runbooks tested
7. âœ… Security scans passing

## Timeline

**Estimated Duration:** 2-3 weeks

---

## Final Notes

This completes the comprehensive coding plan for TravelTomorrow. Each stage builds upon the previous, ensuring a solid foundation for a production-ready travel booking platform with:

- âœ… Scalable infrastructure
- âœ… Robust CI/CD pipelines
- âœ… Comprehensive testing
- âœ… Monitoring and alerting
- âœ… Disaster recovery
- âœ… Operational excellence

**Total Estimated Timeline: 14-18 weeks**

Good luck with the implementation! ðŸš€
